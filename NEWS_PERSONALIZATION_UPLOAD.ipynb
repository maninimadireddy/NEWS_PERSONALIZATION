{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f150441c-bdb6-46ae-94fd-942f21c6b083",
   "metadata": {
    "tags": []
   },
   "source": [
    "## News Personalization: Leveraging RAG for Targeted Content Delivery"
   ]
  },
  {
   "cell_type": "raw",
   "id": "077516bd-4567-4bdb-a816-efb22e13e7d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook leverages LLMs and RAG for personalized news article retrieval and summarization. \n",
    "\n",
    "There are 3 main steps - \n",
    "1. Extracting the archived news dataset from Kaggle\n",
    "2. Encode and upsert the data into a Vector database. (Pinecone vector database)\n",
    "   exercise.\n",
    "3. Leveraging RAG for Targeted Content Delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b6e831-b4a7-4e4f-b2cd-376666452040",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3708d4-a087-4ac5-94df-c1fac27e6d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d2461-25b4-49c4-b6fb-f23bbd3ff90b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    kaggle \\\n",
    "    sagemaker \\\n",
    "    pinecone-client==2.2.1 \\\n",
    "    ipywidgets==7.0.0\\\n",
    "    seaborn\\\n",
    "    sentence-transformers\\\n",
    "    torch==1.13.1 \\\n",
    "    transformers==4.27.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c50d5-f34d-4f0a-b31e-7a66d980f125",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Extracting the archived news dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb7aca-67a7-4c3f-8fcf-f50740da2270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --q kaggle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876fd481-0814-439c-a1dd-ed9baa1feed0",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Note: According to kaggle api documentation the location where credentials json is looking for is ~/.kaggle/kaggle.json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f01da-fac8-4a21-98b3-20491dfec9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!touch ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b64dfb-fc21-44e0-9afc-8cb35a151164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_token={\"username\":\"abc\",\"key\":\"1234\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f376404-c82f-4cfc-bf1f-8452e8881781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cfccc-91d0-401f-9bc0-874599a5d6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d rmisra/news-category-dataset --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af6b2dc-7366-49f3-9fdf-975bab65ab4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()+\"/Data/News_Category_Dataset_v3.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56dd95-d222-4683-bbe5-f564d9e4ec94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### READ IN THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59666ba7-286d-4a75-beb1-b3e360b5263d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(os.getcwd()+\"/Data/News_Category_Dataset_v3.json\",\n",
    "                 lines=True)\n",
    "#print(df.shape)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d52711-6534-491d-94b6-2df6cd80e65c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.groupby('category').agg(_num_articles=('headline','count'),\n",
    "#                           _min_dt=('date','min'),\n",
    "#                          _max_dt=('date','max')).reset_index().sort_values('_num_articles',ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208486d7-1008-4897-bcf0-679f0d585b57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.Encode and upsert the data into a Vector database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c891b8-ffe2-419d-ad94-d925e76459ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup - Sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb17531-c7ce-4169-9997-13a81d96fcbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57162279-3e7a-42ec-b4a2-eb732ccc7c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad42c96a-2abf-4fe0-aa04-f069a1a04011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name='sentence-transformers/all-MiniLM-L6-v2'\n",
    "encoder = SentenceTransformer(model_name_or_path=model_name)\n",
    "\n",
    "### checking sentence encoding\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "embeddings = encoder.encode(sentences)\n",
    "print(f\"Number of sentences embedded = {len(embeddings)}\")\n",
    "print(f\"Length of emberddings embedded = {len((embeddings[0]))}\")\n",
    "print(f\"First 10 elements of the embedding =\\n {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a71f98-87fb-4218-ac3a-c310b98e1d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_subset_test=df.loc[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50be9c6-48e4-4cf3-a02f-851011fe8a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_item_sentence(item: \"pd.Series\", \n",
    "                           text_columns:\"list of columns\") -> str:\n",
    "    \"\"\"\n",
    "    \n",
    "    This function concatenates columns of interest and generates sentence embeddings of the concatenated text.\n",
    "    \n",
    "    Args: \n",
    "        item (pd.Series): row of a pandas dataframe\n",
    "        text_columns (list): list of columns'\n",
    "    Returns:\n",
    "         str: concatenated string\n",
    "    \"\"\"\n",
    "    return ' '.join([item[column] for column in text_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddec08-5710-4c24-9fb3-c50b3ef71819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_subset_test[\"sentence\"] = df_subset_test.apply(lambda row: generate_item_sentence(row,[\"headline\",\"short_description\"]), \n",
    "                                                  axis=1)\n",
    "\n",
    "df_subset_test[\"sentence_embedding\"] = df_subset_test[\"sentence\"].apply(encoder.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20ba9b-7357-41f5-bc40-66df7c46495f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_item_sentence.__annotations__\n",
    "help(generate_item_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414aa188-5e9c-4a8a-b92a-41457c589c64",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PINECONE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2d5e3-eb82-46b0-8ad2-88228acd45d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import os\n",
    "\n",
    "PINECONE_API_KEY=\"234324dsfdsfs\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"PINECONE_API_ENV\"] = \"gcp-starter\"\n",
    "\n",
    "pinecone.init(\n",
    "    api_key = os.environ.get('PINECONE_API_KEY'),\n",
    "    environment = os.environ.get('PINECONE_API_ENV')\n",
    ")\n",
    "\n",
    "#listing all the indexes\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56ced2-2fd3-474e-81c4-bc1a8b4630de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'news-articles-rag-aws'\n",
    "\n",
    "if index_name in pinecone.list_indexes():\n",
    "    pinecone.delete_index(index_name)\n",
    "\n",
    "#Index deleted \n",
    "print(pinecone.list_indexes())\n",
    "    \n",
    "pinecone.create_index(\n",
    "    name=index_name,\n",
    "    dimension=encoder.get_sentence_embedding_dimension(),\n",
    "    metric='cosine'\n",
    ")\n",
    "# wait for index to finish initialization\n",
    "while not pinecone.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed8dab-ada9-4913-957c-8170a4af0077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Checking the indexes creation:\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3591e4a-51bc-4163-b087-18d4276d294f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upsert data into Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456c22f-1507-499a-b3c3-5cf8b0f0b8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 2  # can increase but needs larger instance size otherwise instance runs out of memory\n",
    "vector_limit = df_subset_test.shape[0]#1000\n",
    "\n",
    "answers = df_subset_test[:vector_limit]\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "for i in tqdm(range(0, len(answers), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(i+batch_size, len(answers))\n",
    "    # create IDs batch\n",
    "    ids = [str(x) for x in range(i, i_end)]\n",
    "    if i%100==0:\n",
    "        print(f\"i = {i}, i_end = {i_end}, ids = {ids}\")\n",
    "    # create metadata batch\n",
    "    metadatas = [{'text': text} for text in answers[\"sentence\"][i:i_end]]\n",
    "    #print(\"--------Metadata----------\")\n",
    "    #print(metadatas)\n",
    "    # create embeddings\n",
    "    texts = answers[\"sentence\"][i:i_end].tolist()\n",
    "    #print(\"--------Texts----------\")\n",
    "    #print(texts)\n",
    "    #print(\"--------Embedding----------\")\n",
    "    embeddings=[encoder.encode(sent).tolist() for sent in texts]\n",
    "    #print(f\"Length of embeddings = {len(embeddings)}\")\n",
    "    #embeddings = embed_docs(texts)\n",
    "    #df_subset_test[\"sentence\"].apply(encoder.encode)\n",
    "    # create records list for upsert\n",
    "    #print(\"---------Records------------\")\n",
    "    records = zip(ids, embeddings, metadatas)\n",
    "    #print(f\"records = {records}\")\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458c8d2-b63e-42a6-a87f-1ae19d1fe4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check number of records in the index\n",
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c9505-858e-4fa6-a861-b36aff2ebaee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Leveraging RAG for Targeted Content Delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24215ee-000f-4649-985d-01635a792ef0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc5480-9865-4f40-bdae-19e20071e189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "model_name='google/flan-t5-base'\n",
    "\n",
    "model_flan = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer_flan = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59dd034-fddb-417a-9c88-3b3a300adcd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RAG Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeea6da-ef83-4c53-a3da-7f6cdf8667e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "### checking sentence encoding\n",
    "\n",
    "model_name='sentence-transformers/all-MiniLM-L6-v2'\n",
    "encoder = SentenceTransformer(model_name_or_path=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4f777-15f4-4867-a8d6-1b812cc486e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import os\n",
    "\n",
    "PINECONE_API_KEY=\"2343242adasda\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"PINECONE_API_ENV\"] = \"gcp-starter\"\n",
    "\n",
    "pinecone.init(\n",
    "    api_key = os.environ.get('PINECONE_API_KEY'),\n",
    "    environment = os.environ.get('PINECONE_API_ENV')\n",
    ")\n",
    "\n",
    "#listing all the indexes\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb8b71-3abe-4d9b-a0f9-be182e61ae7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check number of records in the index\n",
    "index_name='news-articles-rag-aws'\n",
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b7f50-9f0c-4143-a3f6-d3f0bbd0c3e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3d483-14d9-4f6b-b29e-957c4bf5b90d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retriver (query_text: str,top_k: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    This function retrieves the relevant articles from the Pinecone vector database.\n",
    "    \n",
    "    Args: \n",
    "         query:str: User query\n",
    "         top_k: int: Top K responses to return\n",
    "    Returns:\n",
    "         list[str]: List of relevant top 5 articles\n",
    "    \"\"\"\n",
    "    \n",
    "    query_vector = encoder.encode(query_text).tolist()\n",
    "\n",
    "    res = index.query(query_vector, top_k=top_k, include_metadata=True)\n",
    "\n",
    "    # show the results\n",
    "    #res\n",
    "\n",
    "    contexts = [match.metadata['text'] for match in res.matches]\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7710dc5-bdfa-4ccf-9963-f81920dc9863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriver.__annotations__\n",
    "help(retriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759f4b7-eb46-4a35-912a-417e10ecb732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "def construct_context(contexts: List[str],max_section_len: int,separator: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    This function generates the context string from RAG response.\n",
    "    \n",
    "    Args: \n",
    "         contexts: List[str]: RAG semantic search response\n",
    "         max_section_len: int: Max length of the context\n",
    "        separator: str: Seperator between the responses ('/s','/n')\n",
    "    Returns:\n",
    "         str: concatenated string\n",
    "    \"\"\"\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "\n",
    "    for text in contexts:\n",
    "        text = text.strip()\n",
    "        # Add contexts until we run out of space.\n",
    "        chosen_sections_len += len(text) + 2\n",
    "        if chosen_sections_len > max_section_len:\n",
    "            break\n",
    "        chosen_sections.append(text)\n",
    "    concatenated_doc = separator.join(chosen_sections)\n",
    "    '''print(\n",
    "        f\"With maximum sequence length {max_section_len}, selected top {len(chosen_sections)} document sections: \\n{concatenated_doc}\"\n",
    "    )'''\n",
    "    return concatenated_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae27cfe-3e51-499c-8f3a-325bcc09ae0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "construct_context.__annotations__\n",
    "help(construct_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad5c17-885b-4202-9d24-a62854baed98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_payload(prompt_template: str,\n",
    "                      question: str,\n",
    "                      context_str: str,\n",
    "                      padding:str=\"longest\")-> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    This function contructs the prompt for the LLM.\n",
    "    \n",
    "    Args: \n",
    "        prompt_template: str: Input prompt template\n",
    "        question:str: LLM question\n",
    "        context_str: LLM input context information\n",
    "        max_source_length:int: max source length\n",
    "        max_target_length:int:=round(max_source_length/2,0)\n",
    "        padding:str=\"longest\"\n",
    "    Returns:\n",
    "         str: LLM prompt\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.replace(\"{context}\", context_str).replace(\"{question}\", question)\n",
    "\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e40290-d81d-4aa4-8bd8-1d756085eb51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "construct_payload.__annotations__\n",
    "help(construct_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bf695-479d-4f0a-8ba5-031ed0885e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Answer the following QUESTION without hallucination.\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b895468-effa-4a88-adad-95f5112d8c71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LLM Base response without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47242250-8a3a-47fb-a2db-32ebbeee7400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_source_length=512\n",
    "max_target_length=round(max_source_length/2,0)\n",
    "padding=\"longest\"#\"max_length\" #\"longest\"\n",
    "#input_text=example.text[0]\n",
    "\n",
    "question= \"What is the news about air travel?\"#\"Summarize the text without any hallucination:\"#\"Who are the entities:\" #\"What is the sentiment:\" #\"Provide accurate summarization:\"\n",
    "#Summary\n",
    "\n",
    "context_str=\"\"\n",
    "\n",
    "prompt=construct_payload(prompt_template,question,context_str,padding=\"longest\")\n",
    "\n",
    "#print(prompt)\n",
    "\n",
    "inputs = tokenizer_flan(prompt,\n",
    "    max_length=max_source_length,\n",
    "    return_tensors='pt',\n",
    "    padding=padding,\n",
    "    truncation=True)\n",
    "\n",
    "#max_target_length=max(round(len_input_text/2,0),max_target_length)\n",
    "#print(f\"\\nLENGTH OF INPUT TEXT = {len(prompt)}, max_target_length = {max_target_length}\")\n",
    "base_output=tokenizer_flan.decode(model_flan.generate(inputs[\"input_ids\"]\n",
    "                                                      ,max_new_tokens=max_target_length)[0])\n",
    "base_output=base_output.replace(\"<pad> \", \"\").replace(\"</s>\", \"\")\n",
    "\n",
    "print(f'LLM RESPONSE WITHOUT RAG CONTEXT:\\n{base_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9734d-6ea0-4bb2-91cf-498497879931",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LLM response with RAG - FLAN-T5-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea9c7d-b4a7-4caf-86af-4feb7bc37a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_source_length=512\n",
    "max_target_length=1000#max_source_length\n",
    "padding=\"longest\"#\"max_length\" #\"longest\"\n",
    "#input_text=example.text[0]\n",
    "\n",
    "question= \"What is the news about air travel?\"#\"Summarize the text without any hallucination:\"#\"Who are the entities:\" #\"What is the sentiment:\" #\"Provide accurate summarization:\"\n",
    "\n",
    "#Generate the contexts\n",
    "contexts=retriver(query_text = question,top_k=5)\n",
    "print(f\"{contexts=}\")\n",
    "\n",
    "#Construct the context\n",
    "context_str = construct_context(contexts=contexts,max_section_len = 2000,separator = '\\n')#\"\\n\")\n",
    "print(f\"\\n{context_str=}\")\n",
    "\n",
    "\n",
    "#Create the prompt\n",
    "prompt=construct_payload(prompt_template,question,context_str,padding=\"longest\")\n",
    "print(f\"\\n{prompt=}\")\n",
    "\n",
    "\n",
    "inputs = tokenizer_flan(prompt,\n",
    "    max_length=max_source_length,\n",
    "    return_tensors='pt',\n",
    "    padding=padding,\n",
    "    truncation=True)\n",
    "\n",
    "rag_output=tokenizer_flan.decode(model_flan.generate(inputs[\"input_ids\"],max_new_tokens=max_target_length)[0])\n",
    "rag_output=rag_output.replace(\"<pad> \", \"\").replace(\"</s>\", \"\")\n",
    "\n",
    "print(f'\\n\\nLLM RESPONSE WITH RAG CONTEXT:\\n{rag_output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000c23fb-6f5c-4240-8b24-a53b2c29aee2",
   "metadata": {},
   "source": [
    "### LLM response with RAG - FLAN-T5-Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed9780-4f01-4a2c-9929-be68c3b1ab04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_flan_small = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model_flan_small = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbae1d0-38e0-4aae-bc10-c8698906780f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_source_length=512\n",
    "max_target_length=1000#max_source_length\n",
    "padding=\"longest\"#\"max_length\" #\"longest\"\n",
    "#input_text=example.text[0]\n",
    "\n",
    "question= \"What is the news about air travel?\"#\"Summarize the text without any hallucination:\"#\"Who are the entities:\" #\"What is the sentiment:\" #\"Provide accurate summarization:\"\n",
    "\n",
    "#Generate the contexts\n",
    "contexts=retriver(query_text = question,top_k=5)\n",
    "print(f\"{contexts=}\")\n",
    "\n",
    "#Construct the context\n",
    "context_str = construct_context(contexts=contexts,max_section_len = 2000,separator = '\\n')#\"\\n\")\n",
    "print(f\"\\n{context_str=}\")\n",
    "\n",
    "\n",
    "#Create the prompt\n",
    "prompt=construct_payload(prompt_template,question,context_str,padding=\"longest\")\n",
    "print(f\"\\n{prompt=}\")\n",
    "\n",
    "\n",
    "inputs = tokenizer_flan_small(prompt,\n",
    "    max_length=max_source_length,\n",
    "    return_tensors='pt',\n",
    "    padding=padding,\n",
    "    truncation=True)\n",
    "\n",
    "rag_output=tokenizer_flan_small.decode(model_flan_small.generate(inputs[\"input_ids\"],max_new_tokens=max_target_length)[0])\n",
    "#rag_output=rag_output.replace(\"<pad> \", \"\").replace(\"</s>\", \"\")\n",
    "\n",
    "print(f'\\n\\nLLM RESPONSE WITH RAG CONTEXT:\\n{rag_output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74df0a-133e-43ec-a40e-bbaa4d9618e7",
   "metadata": {},
   "source": [
    "### LLM response with RAG - Falconsai/text_summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666074d-70c3-4224-b3cc-7a6dd74af6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_falcon = AutoTokenizer.from_pretrained(\"Falconsai/text_summarization\")\n",
    "model_falcon = AutoModelForSeq2SeqLM.from_pretrained(\"Falconsai/text_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded23a15-5a36-48cb-a547-fc19986b36d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "max_source_length=2000\n",
    "max_target_length=20000\n",
    "\n",
    "#Generate the contexts\n",
    "contexts=retriver(query_text = question,top_k=5)\n",
    "print(f\"{contexts=}\")\n",
    "\n",
    "#Construct the context\n",
    "context_str = construct_context(contexts=contexts,\n",
    "                                max_section_len = 2000,separator = '\\n')#\"\\n\")\n",
    "print(f\"\\n{context_str=}\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
    "\n",
    "rag_sumamrizer=summarizer(context_str, max_length=len(context_str)+10, min_length=30, do_sample=False)[0]['summary_text']\n",
    "\n",
    "print(f\"\\n\\nLLM RESPONSE WITH RAG CONTEXT USING Falconsai/text_summarization =\\n{rag_sumamrizer}\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
